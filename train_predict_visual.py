#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""å­¦ç¿’ãƒ»äºˆæ¸¬å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå¯è¦–åŒ–æ©Ÿèƒ½ä»˜ãï¼‰"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import optuna
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
from datetime import datetime
from dotenv import load_dotenv
import warnings
warnings.filterwarnings('ignore')

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ—ãƒ­ãƒƒãƒˆã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

def visualize_data_distribution(df, target_col='Actual Value', output_dir='output_data/visualizations'):
    """ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å¯è¦–åŒ–"""
    os.makedirs(output_dir, exist_ok=True)

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 1. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
    axes[0, 0].hist(df[target_col].dropna(), bins=50, color='skyblue', edgecolor='black', alpha=0.7)
    axes[0, 0].set_title('Distribution of Actual Value', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('Actual Value')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].grid(True, alpha=0.3)

    # 2. ãƒ­ã‚°å¤‰æ›å¾Œã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    log_values = np.log1p(df[target_col].clip(lower=0)).dropna()
    axes[0, 1].hist(log_values, bins=50, color='lightcoral', edgecolor='black', alpha=0.7)
    axes[0, 1].set_title('Distribution of log1p(Actual Value)', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('log1p(Actual Value)')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].grid(True, alpha=0.3)

    # 3. æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ
    if 'File Date' in df.columns:
        df_sorted = df.sort_values('File Date')
        daily_sum = df_sorted.groupby('File Date')[target_col].sum()
        axes[1, 0].plot(daily_sum.index, daily_sum.values, color='green', alpha=0.7)
        axes[1, 0].set_title('Daily Total of Actual Value', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Date')
        axes[1, 0].set_ylabel('Total Actual Value')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)

    # 4. ç®±ã²ã’å›³ï¼ˆæœˆåˆ¥ï¼‰
    if 'month' in df.columns:
        monthly_data = [df[df['month'] == m][target_col].dropna() for m in sorted(df['month'].unique())]
        axes[1, 1].boxplot(monthly_data, labels=sorted(df['month'].unique()))
        axes[1, 1].set_title('Monthly Distribution of Actual Value', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Month')
        axes[1, 1].set_ylabel('Actual Value')
        axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f"{output_dir}/data_distribution.png", dpi=150, bbox_inches='tight')
    plt.show()

    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å¯è¦–åŒ–å®Œäº†")

def visualize_seasonal_patterns(df, target_col='Actual Value', output_dir='output_data/visualizations'):
    """å­£ç¯€æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–"""
    os.makedirs(output_dir, exist_ok=True)

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 1. æœˆåˆ¥å¹³å‡
    if 'month' in df.columns:
        monthly_avg = df.groupby('month')[target_col].mean()
        axes[0, 0].bar(monthly_avg.index, monthly_avg.values, color='steelblue')
        axes[0, 0].set_title('Average by Month', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Month')
        axes[0, 0].set_ylabel('Average Actual Value')
        axes[0, 0].grid(True, alpha=0.3)

    # 2. æ›œæ—¥åˆ¥å¹³å‡
    if 'day_of_week' in df.columns:
        weekday_avg = df.groupby('day_of_week')[target_col].mean()
        weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
        axes[0, 1].bar(weekday_avg.index, weekday_avg.values, color='coral')
        axes[0, 1].set_title('Average by Weekday', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Day of Week')
        axes[0, 1].set_ylabel('Average Actual Value')
        axes[0, 1].set_xticks(range(7))
        axes[0, 1].set_xticklabels(weekday_names)
        axes[0, 1].grid(True, alpha=0.3)

    # 3. æ—¥åˆ¥å¹³å‡ï¼ˆ1-31æ—¥ï¼‰
    if 'day' in df.columns:
        daily_avg = df.groupby('day')[target_col].mean()
        axes[1, 0].plot(daily_avg.index, daily_avg.values, marker='o', color='green')
        axes[1, 0].set_title('Average by Day of Month', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Day')
        axes[1, 0].set_ylabel('Average Actual Value')
        axes[1, 0].grid(True, alpha=0.3)

    # 4. å¹´åˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰
    if 'year' in df.columns:
        yearly_sum = df.groupby('year')[target_col].sum()
        axes[1, 1].bar(yearly_sum.index, yearly_sum.values, color='purple')
        axes[1, 1].set_title('Yearly Total', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Year')
        axes[1, 1].set_ylabel('Total Actual Value')
        axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f"{output_dir}/seasonal_patterns.png", dpi=150, bbox_inches='tight')
    plt.show()

    print("ğŸ“Š å­£ç¯€æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–å®Œäº†")

def visualize_model_performance(y_true, y_pred, model_name="LightGBM", output_dir='output_data/visualizations'):
    """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®å¯è¦–åŒ–"""
    os.makedirs(output_dir, exist_ok=True)

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))

    # 1. å®Ÿæ¸¬å€¤ vs äºˆæ¸¬å€¤
    axes[0, 0].scatter(y_true, y_pred, alpha=0.5, s=10)
    axes[0, 0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
    axes[0, 0].set_title(f'{model_name}: Actual vs Predicted', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('Actual Value')
    axes[0, 0].set_ylabel('Predicted Value')
    axes[0, 0].grid(True, alpha=0.3)

    # 2. æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
    residuals = y_true - y_pred
    axes[0, 1].scatter(y_pred, residuals, alpha=0.5, s=10)
    axes[0, 1].axhline(y=0, color='r', linestyle='--', lw=2)
    axes[0, 1].set_title('Residual Plot', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('Predicted Value')
    axes[0, 1].set_ylabel('Residuals')
    axes[0, 1].grid(True, alpha=0.3)

    # 3. æ®‹å·®ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    axes[0, 2].hist(residuals, bins=50, edgecolor='black', alpha=0.7)
    axes[0, 2].set_title('Distribution of Residuals', fontsize=14, fontweight='bold')
    axes[0, 2].set_xlabel('Residuals')
    axes[0, 2].set_ylabel('Frequency')
    axes[0, 2].axvline(x=0, color='r', linestyle='--', lw=2)
    axes[0, 2].grid(True, alpha=0.3)

    # 4. Q-Qãƒ—ãƒ­ãƒƒãƒˆ
    from scipy import stats
    stats.probplot(residuals, dist="norm", plot=axes[1, 0])
    axes[1, 0].set_title('Q-Q Plot', fontsize=14, fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)

    # 5. äºˆæ¸¬èª¤å·®ã®åˆ†å¸ƒ
    percentage_error = np.abs(residuals) / (np.abs(y_true) + 1e-10) * 100
    axes[1, 1].hist(percentage_error, bins=50, edgecolor='black', alpha=0.7)
    axes[1, 1].set_title('Distribution of Percentage Error', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('Percentage Error (%)')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].axvline(x=np.median(percentage_error), color='r', linestyle='--', lw=2, label=f'Median: {np.median(percentage_error):.2f}%')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    # 6. ç´¯ç©èª¤å·®
    cumulative_error = np.cumsum(residuals)
    axes[1, 2].plot(cumulative_error, color='blue')
    axes[1, 2].set_title('Cumulative Error', fontsize=14, fontweight='bold')
    axes[1, 2].set_xlabel('Sample Index')
    axes[1, 2].set_ylabel('Cumulative Error')
    axes[1, 2].axhline(y=0, color='r', linestyle='--', lw=2)
    axes[1, 2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f"{output_dir}/model_performance.png", dpi=150, bbox_inches='tight')
    plt.show()

    print("ğŸ“Š ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®å¯è¦–åŒ–å®Œäº†")

def visualize_feature_importance(importance_df, top_n=20, output_dir='output_data/visualizations'):
    """ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–"""
    os.makedirs(output_dir, exist_ok=True)

    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # Top Nç‰¹å¾´é‡ã‚’å–å¾—
    top_features = importance_df.nlargest(top_n, 'importance')

    # 1. æ£’ã‚°ãƒ©ãƒ•
    axes[0].barh(range(len(top_features)), top_features['importance'].values, color='steelblue')
    axes[0].set_yticks(range(len(top_features)))
    axes[0].set_yticklabels(top_features['feature'].values)
    axes[0].set_title(f'Top {top_n} Feature Importances', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Importance')
    axes[0].grid(True, alpha=0.3)
    axes[0].invert_yaxis()

    # 2. ç´¯ç©é‡è¦åº¦
    importance_sorted = importance_df.sort_values('importance', ascending=False)
    cumsum = importance_sorted['importance'].cumsum() / importance_sorted['importance'].sum() * 100

    axes[1].plot(range(1, len(cumsum) + 1), cumsum.values, marker='o', color='green')
    axes[1].set_title('Cumulative Feature Importance', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Number of Features')
    axes[1].set_ylabel('Cumulative Importance (%)')
    axes[1].axhline(y=80, color='r', linestyle='--', lw=2, label='80% threshold')
    axes[1].axhline(y=90, color='orange', linestyle='--', lw=2, label='90% threshold')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f"{output_dir}/feature_importance.png", dpi=150, bbox_inches='tight')
    plt.show()

    print("ğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–å®Œäº†")

def print_evaluation_metrics(y_true, y_pred, model_name="Model"):
    """è©•ä¾¡æŒ‡æ¨™ã®è©³ç´°è¡¨ç¤º"""

    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    # MAPEã®è¨ˆç®—ï¼ˆ0é™¤ç®—ã‚’å›é¿ï¼‰
    mask = y_true != 0
    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if mask.any() else np.inf

    # ä¸­å¤®å€¤çµ¶å¯¾èª¤å·®
    median_ae = np.median(np.abs(y_true - y_pred))

    # èª¤å·®ã®æ¨™æº–åå·®
    std_error = np.std(y_true - y_pred)

    print("\n" + "="*60)
    print(f"ğŸ“Š {model_name} - è©•ä¾¡æŒ‡æ¨™ã‚µãƒãƒªãƒ¼")
    print("="*60)
    print(f"âœ… RMSE (Root Mean Square Error):     {rmse:,.4f}")
    print(f"âœ… MAE (Mean Absolute Error):          {mae:,.4f}")
    print(f"âœ… RÂ² Score:                           {r2:.4f}")
    print(f"âœ… MAPE (Mean Absolute % Error):       {mape:.2f}%")
    print(f"âœ… Median Absolute Error:              {median_ae:,.4f}")
    print(f"âœ… Standard Deviation of Error:        {std_error:,.4f}")
    print("-"*60)

    # çµ±è¨ˆã‚µãƒãƒªãƒ¼
    print("\nğŸ“ˆ äºˆæ¸¬å€¤ã®çµ±è¨ˆã‚µãƒãƒªãƒ¼:")
    print(f"   æœ€å°å€¤:     {y_pred.min():,.2f}")
    print(f"   25%åˆ†ä½æ•°:  {np.percentile(y_pred, 25):,.2f}")
    print(f"   ä¸­å¤®å€¤:     {np.median(y_pred):,.2f}")
    print(f"   75%åˆ†ä½æ•°:  {np.percentile(y_pred, 75):,.2f}")
    print(f"   æœ€å¤§å€¤:     {y_pred.max():,.2f}")
    print(f"   å¹³å‡å€¤:     {y_pred.mean():,.2f}")
    print(f"   æ¨™æº–åå·®:   {y_pred.std():,.2f}")

    # èª¤å·®ã®çµ±è¨ˆã‚µãƒãƒªãƒ¼
    errors = y_true - y_pred
    print("\nğŸ“‰ èª¤å·®ã®çµ±è¨ˆã‚µãƒãƒªãƒ¼:")
    print(f"   æœ€å°èª¤å·®:   {errors.min():,.2f}")
    print(f"   25%åˆ†ä½æ•°:  {np.percentile(errors, 25):,.2f}")
    print(f"   ä¸­å¤®å€¤:     {np.median(errors):,.2f}")
    print(f"   75%åˆ†ä½æ•°:  {np.percentile(errors, 75):,.2f}")
    print(f"   æœ€å¤§èª¤å·®:   {errors.max():,.2f}")
    print("="*60)

    return {
        'rmse': rmse,
        'mae': mae,
        'r2': r2,
        'mape': mape,
        'median_ae': median_ae,
        'std_error': std_error
    }

def train_model_with_visualization(df_features, target_col='Actual Value'):
    """å¯è¦–åŒ–æ©Ÿèƒ½ä»˜ãã®ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""

    print("\n" + "="*60)
    print("ğŸš€ æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
    print("="*60)

    # ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–
    print("\nğŸ“Š Step 1: ãƒ‡ãƒ¼ã‚¿åˆ†æã¨å¯è¦–åŒ–")
    print("-"*40)
    visualize_data_distribution(df_features, target_col)
    visualize_seasonal_patterns(df_features, target_col)

    # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’é¸æŠ
    feature_cols = [col for col in df_features.columns
                   if col.endswith('_f') or col in ['year', 'month', 'day', 'day_of_week']]

    # æ•°å€¤å‹ã®ã¿ã‚’é¸æŠ
    numeric_cols = []
    for col in feature_cols:
        if col in df_features.columns:
            df_features[col] = pd.to_numeric(df_features[col], errors='coerce')
            numeric_cols.append(col)

    feature_cols = numeric_cols

    print(f"\nğŸ” === ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡è©³ç´° ===")
    print(f"âœ… ç‰¹å¾´é‡æ•°: {len(feature_cols)}å€‹")
    print(f"âœ… ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ: {feature_cols}")

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®å‡¦ç†
    if target_col in df_features.columns:
        df_features[target_col] = pd.to_numeric(df_features[target_col], errors='coerce').fillna(0)
    else:
        print(f"ã‚¨ãƒ©ãƒ¼: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•° '{target_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None, None, None

    # NaNã‚’å‰Šé™¤
    df_clean = df_features[feature_cols + [target_col]].dropna()
    print(f"âœ… ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df_clean.shape}")

    # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
    X = df_clean[feature_cols]
    y = df_clean[target_col]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, shuffle=False  # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãªã®ã§shuffle=False
    )

    print(f"âœ… å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {X_train.shape}")
    print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}")

    # LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    print("\nğŸ“Š Step 2: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆLightGBMï¼‰")
    print("-"*40)

    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'n_estimators': 100,
        'random_state': 42
    }

    model = lgb.LGBMRegressor(**params)

    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å­¦ç¿’éç¨‹ã‚’ç›£è¦–
    eval_set = [(X_test, y_test)]
    model.fit(
        X_train, y_train,
        eval_set=eval_set,
        callbacks=[
            lgb.early_stopping(10),
            lgb.log_evaluation(20)
        ]
    )

    print(f"âœ… æœ€é©ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {model.n_estimators_}")

    # äºˆæ¸¬
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    # è©•ä¾¡
    print("\nğŸ“Š Step 3: ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
    print("-"*40)

    print("\nã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®è©•ä¾¡ã€‘")
    train_metrics = print_evaluation_metrics(y_train.values, y_pred_train, "Training Set")

    print("\nã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è©•ä¾¡ã€‘")
    test_metrics = print_evaluation_metrics(y_test.values, y_pred_test, "Test Set")

    # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®å¯è¦–åŒ–
    print("\nğŸ“Š Step 4: çµæœã®å¯è¦–åŒ–")
    print("-"*40)
    visualize_model_performance(y_test.values, y_pred_test, "LightGBM")

    # ç‰¹å¾´é‡é‡è¦åº¦
    importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    print("\nğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ TOP10:")
    print("-"*40)
    for idx, row in importance.head(10).iterrows():
        print(f"  {row['feature']:15s} : {row['importance']:8.1f}")

    visualize_feature_importance(importance)

    return model, importance, test_metrics

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""

    print("\n" + "="*70)
    print("ğŸ¯ ãƒ¤ãƒã‚µç¢ºå®šæ³¨æ–‡äºˆæ¸¬ - æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå¯è¦–åŒ–ç‰ˆï¼‰")
    print("="*70)

    # ãƒ­ãƒ¼ã‚«ãƒ«ã®ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    features_file = "output_data/features/df_features_yamasa_latest.parquet"

    if not os.path.exists(features_file):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {features_file}")
        print("   å…ˆã« create_features_local.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return None

    print(f"\nğŸ“‚ ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿èª­è¾¼ä¸­...")
    print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {features_file}")

    df_features = pd.read_parquet(features_file)
    print(f"âœ… èª­è¾¼å®Œäº†: {df_features.shape}")

    # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±ã‚’è¡¨ç¤º
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
    print("-"*40)
    print(f"  è¡Œæ•°: {len(df_features):,}")
    print(f"  ã‚«ãƒ©ãƒ æ•°: {len(df_features.columns)}")

    # Material Keyæ¯ã®ãƒ‡ãƒ¼ã‚¿æ•°
    if 'Material Key' in df_features.columns:
        n_materials = df_features['Material Key'].nunique()
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªMaterial Keyæ•°: {n_materials:,}")

    # æ—¥ä»˜ç¯„å›²
    if 'File Date' in df_features.columns:
        df_features['File Date'] = pd.to_datetime(df_features['File Date'], errors='coerce')
        date_min = df_features['File Date'].min()
        date_max = df_features['File Date'].max()
        print(f"  æ—¥ä»˜ç¯„å›²: {date_min.strftime('%Y-%m-%d')} ã€œ {date_max.strftime('%Y-%m-%d')}")

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨å¯è¦–åŒ–
    model, importance, metrics = train_model_with_visualization(df_features)

    if model is None:
        print("âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return None

    # çµæœã‚’ä¿å­˜
    output_dir = "output_data/models"
    os.makedirs(output_dir, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    import joblib
    model_file = f"{output_dir}/model_visual_{timestamp}.pkl"
    joblib.dump(model, model_file)
    print(f"\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")

    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’ä¿å­˜
    importance_file = f"{output_dir}/importance_visual_{timestamp}.csv"
    importance.to_csv(importance_file, index=False)
    print(f"ğŸ’¾ ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜: {importance_file}")

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜
    metrics_file = f"{output_dir}/metrics_visual_{timestamp}.json"
    pd.Series(metrics).to_json(metrics_file)
    print(f"ğŸ’¾ è©•ä¾¡æŒ‡æ¨™ä¿å­˜: {metrics_file}")

    print("\n" + "="*70)
    print("âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("="*70)
    print("\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"  - ãƒ¢ãƒ‡ãƒ«: {model_file}")
    print(f"  - é‡è¦åº¦: {importance_file}")
    print(f"  - è©•ä¾¡æŒ‡æ¨™: {metrics_file}")
    print(f"  - å¯è¦–åŒ–ç”»åƒ: output_data/visualizations/*.png")

    return model, importance, metrics

if __name__ == "__main__":
    main()