#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""å­¦ç¿’ãƒ»äºˆæ¸¬å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆäºˆæ¸¬èª¤å·®åˆ†ææ©Ÿèƒ½ä»˜ãï¼‰"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import optuna
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import os
from datetime import datetime
from dotenv import load_dotenv
import boto3
from io import BytesIO
import pickle

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

def calculate_prediction_errors(y_true, y_pred, test_data, material_key_col='material_key', file_date_col='file_date'):
    """äºˆæ¸¬èª¤å·®ã‚’è¨ˆç®—

    Args:
        y_true: å®Ÿç¸¾å€¤
        y_pred: äºˆæ¸¬å€¤
        test_data: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆMaterial Keyã¨File Dateã‚’å«ã‚€ï¼‰
        material_key_col: Material Keyã®ã‚«ãƒ©ãƒ å
        file_date_col: File Dateã®ã‚«ãƒ©ãƒ å

    Returns:
        èª¤å·®åˆ†æçµæœã®DataFrame
    """

    # çµæœã‚’DataFrameã«ã¾ã¨ã‚ã‚‹
    df_results = pd.DataFrame({
        material_key_col: test_data[material_key_col].values,
        file_date_col: test_data[file_date_col].values,
        'actual': y_true.values,
        'predicted': y_pred,
    })

    # èª¤å·®ç‡ã‚’è¨ˆç®—ï¼ˆå®Ÿç¸¾å€¤ãŒ0ã§ãªã„å ´åˆã®ã¿ï¼‰
    df_results['error_rate'] = np.where(
        df_results['actual'] != 0,
        (df_results['predicted'] - df_results['actual']) / df_results['actual'],
        np.nan
    )

    # çµ¶å¯¾èª¤å·®ç‡ã‚‚è¨ˆç®—
    df_results['abs_error_rate'] = np.abs(df_results['error_rate'])

    # Material Key Ã— File Dateæ¯ã®é›†è¨ˆ
    df_key_date = df_results.groupby([material_key_col, file_date_col]).agg({
        'actual': 'sum',
        'predicted': 'sum',
        'error_rate': lambda x: x[~x.isna()].mean() if (~x.isna()).any() else np.nan,
        'abs_error_rate': lambda x: x[~x.isna()].mean() if (~x.isna()).any() else np.nan
    }).reset_index()

    # Material Key Ã— File Dateæ¯ã®èª¤å·®ç‡ã‚’å†è¨ˆç®—ï¼ˆåˆè¨ˆå€¤ãƒ™ãƒ¼ã‚¹ï¼‰
    df_key_date['error_rate_sum'] = np.where(
        df_key_date['actual'] != 0,
        (df_key_date['predicted'] - df_key_date['actual']) / df_key_date['actual'],
        np.nan
    )

    # å®Ÿç¸¾å€¤>0ã®ã‚«ã‚¦ãƒ³ãƒˆï¼ˆMaterial Key Ã— File Dateæ¯ï¼‰
    df_count = df_results[df_results['actual'] > 0].groupby([material_key_col, file_date_col]).size().reset_index(name='count_actual_positive')
    df_key_date = df_key_date.merge(df_count, on=[material_key_col, file_date_col], how='left')
    df_key_date['count_actual_positive'] = df_key_date['count_actual_positive'].fillna(0).astype(int)

    # Material Keyæ¯ã®é›†è¨ˆï¼ˆ6ãƒ¶æœˆå…¨ä½“ï¼‰
    df_key_total = df_results.groupby(material_key_col).agg({
        'actual': 'sum',
        'predicted': 'sum',
        'error_rate': lambda x: x[~x.isna()].mean() if (~x.isna()).any() else np.nan,
        'abs_error_rate': lambda x: x[~x.isna()].mean() if (~x.isna()).any() else np.nan
    }).reset_index()

    # Material Keyæ¯ã®èª¤å·®ç‡ã‚’å†è¨ˆç®—ï¼ˆåˆè¨ˆå€¤ãƒ™ãƒ¼ã‚¹ï¼‰
    df_key_total['error_rate_total'] = np.where(
        df_key_total['actual'] != 0,
        (df_key_total['predicted'] - df_key_total['actual']) / df_key_total['actual'],
        np.nan
    )

    # å®Ÿç¸¾å€¤>0ã®ã‚«ã‚¦ãƒ³ãƒˆï¼ˆMaterial Keyæ¯ã€6ãƒ¶æœˆå…¨ä½“ï¼‰
    df_count_total = df_results[df_results['actual'] > 0].groupby(material_key_col).size().reset_index(name='count_actual_positive_total')
    df_key_total = df_key_total.merge(df_count_total, on=material_key_col, how='left')
    df_key_total['count_actual_positive_total'] = df_key_total['count_actual_positive_total'].fillna(0).astype(int)

    # key_mean_abs_err_div_pred_overall ã®è¨ˆç®—
    # å„Material Keyã”ã¨ã«: mean(|actual - predicted|) / mean(predicted)
    df_key_metrics = df_results.groupby(material_key_col).agg({
        'actual': 'mean',
        'predicted': 'mean'
    }).reset_index()
    df_key_metrics.columns = [material_key_col, 'actual_mean', 'predicted_mean']

    # çµ¶å¯¾èª¤å·®ã®å¹³å‡ã‚’è¨ˆç®—
    df_abs_error = df_results.copy()
    df_abs_error['abs_error'] = np.abs(df_abs_error['actual'] - df_abs_error['predicted'])
    df_key_abs_error = df_abs_error.groupby(material_key_col)['abs_error'].mean().reset_index()
    df_key_abs_error.columns = [material_key_col, 'mean_abs_error']

    # ãƒãƒ¼ã‚¸ã—ã¦è¨ˆç®—
    df_key_metrics = df_key_metrics.merge(df_key_abs_error, on=material_key_col)
    df_key_metrics['key_mean_abs_err_div_pred_overall'] = np.where(
        df_key_metrics['predicted_mean'] != 0,
        df_key_metrics['mean_abs_error'] / df_key_metrics['predicted_mean'],
        np.nan
    )

    # df_key_totalã«ãƒãƒ¼ã‚¸
    df_key_total = df_key_total.merge(
        df_key_metrics[[material_key_col, 'key_mean_abs_err_div_pred_overall']],
        on=material_key_col,
        how='left'
    )

    return df_results, df_key_date, df_key_total

def analyze_error_distribution(df_key_total):
    """èª¤å·®ç‡ã®åˆ†å¸ƒã‚’åˆ†æ

    Args:
        df_key_total: Material Keyæ¯ã®é›†è¨ˆçµæœ

    Returns:
        èª¤å·®ç‡åˆ¥ã®çµ±è¨ˆ
    """

    # çµ¶å¯¾èª¤å·®ç‡ã§åˆ¤å®šï¼ˆå¾“æ¥ã®æŒ‡æ¨™ï¼‰
    df_valid = df_key_total[~df_key_total['error_rate_total'].isna()].copy()
    df_valid['abs_error_rate_total'] = np.abs(df_valid['error_rate_total'])

    total_materials = len(df_valid)

    # èª¤å·®ç‡åˆ¥ã®ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå¾“æ¥ã®æŒ‡æ¨™ï¼‰
    within_20 = (df_valid['abs_error_rate_total'] <= 0.2).sum()
    within_30 = (df_valid['abs_error_rate_total'] <= 0.3).sum()
    within_50 = (df_valid['abs_error_rate_total'] <= 0.5).sum()

    # èª¤å·®ç‡ã®çµ±è¨ˆå€¤
    error_mean = df_valid['abs_error_rate_total'].mean()
    error_median = df_valid['abs_error_rate_total'].median()
    error_std = df_valid['abs_error_rate_total'].std()

    # æ–°ã—ã„è©•ä¾¡æŒ‡æ¨™: key_mean_abs_err_div_pred_overall
    df_key_eval = df_key_total[~df_key_total['key_mean_abs_err_div_pred_overall'].isna()].copy()
    total_keys_eval = len(df_key_eval)

    # key_mean_abs_err_div_pred_overall ãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡
    eval_mean = df_key_eval['key_mean_abs_err_div_pred_overall'].mean()
    eval_median = df_key_eval['key_mean_abs_err_div_pred_overall'].median()
    eval_within_20 = (df_key_eval['key_mean_abs_err_div_pred_overall'] <= 0.2).sum()
    eval_within_30 = (df_key_eval['key_mean_abs_err_div_pred_overall'] <= 0.3).sum()
    eval_within_50 = (df_key_eval['key_mean_abs_err_div_pred_overall'] <= 0.5).sum()

    stats = {
        'total_materials': total_materials,
        'within_20_percent': within_20,
        'within_30_percent': within_30,
        'within_50_percent': within_50,
        'within_20_percent_ratio': within_20 / total_materials if total_materials > 0 else 0,
        'within_30_percent_ratio': within_30 / total_materials if total_materials > 0 else 0,
        'within_50_percent_ratio': within_50 / total_materials if total_materials > 0 else 0,
        'error_mean': error_mean,
        'error_median': error_median,
        'error_std': error_std,
        # æ–°ã—ã„è©•ä¾¡æŒ‡æ¨™
        'eval_total_keys': total_keys_eval,
        'eval_mean': eval_mean,
        'eval_median': eval_median,
        'eval_within_20': eval_within_20,
        'eval_within_30': eval_within_30,
        'eval_within_50': eval_within_50,
        'eval_within_20_ratio': eval_within_20 / total_keys_eval if total_keys_eval > 0 else 0,
        'eval_within_30_ratio': eval_within_30 / total_keys_eval if total_keys_eval > 0 else 0,
        'eval_within_50_ratio': eval_within_50 / total_keys_eval if total_keys_eval > 0 else 0
    }

    return stats

def train_and_predict_with_test_period(df_features, test_start='2025-01-01', test_end='2025-06-30', target_col='actual_value', is_optuna=True):
    """æŒ‡å®šæœŸé–“ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨äºˆæ¸¬ã‚’å®Ÿè¡Œ

    Args:
        df_features: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        test_start: ãƒ†ã‚¹ãƒˆæœŸé–“é–‹å§‹æ—¥
        test_end: ãƒ†ã‚¹ãƒˆæœŸé–“çµ‚äº†æ—¥
        target_col: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ã‚«ãƒ©ãƒ å
        is_optuna: Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’è¡Œã†ã‹ã©ã†ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
    """

    # æ—¥ä»˜ã‚«ãƒ©ãƒ ã‚’ datetimeå‹ã«å¤‰æ›
    df_features['file_date'] = pd.to_datetime(df_features['file_date'], errors='coerce')

    # ãƒ†ã‚¹ãƒˆæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é›¢
    test_mask = (df_features['file_date'] >= test_start) & (df_features['file_date'] <= test_end)
    df_train = df_features[~test_mask].copy()
    df_test = df_features[test_mask].copy()

    print(f"\n=== ãƒ‡ãƒ¼ã‚¿åˆ†å‰² ===")
    print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df_train['file_date'].min()} ã€œ {df_train['file_date'].max()}")
    print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df_train.shape}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æœŸé–“: {df_test['file_date'].min()} ã€œ {df_test['file_date'].max()}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df_test.shape}")

    # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’é¸æŠï¼ˆ_fã§çµ‚ã‚ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ï¼‰
    feature_cols = [col for col in df_features.columns if col.endswith('_f')]

    # æ•°å€¤å‹ã®ã¿ã‚’é¸æŠ
    numeric_cols = []
    for col in feature_cols:
        if col in df_features.columns:
            df_train[col] = pd.to_numeric(df_train[col], errors='coerce')
            df_test[col] = pd.to_numeric(df_test[col], errors='coerce')
            numeric_cols.append(col)

    feature_cols = numeric_cols
    print(f"\n=== ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡: {len(feature_cols)}å€‹ ===")
    print(f"ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ: {feature_cols}")

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®å‡¦ç†
    if target_col in df_train.columns:
        df_train[target_col] = pd.to_numeric(df_train[target_col], errors='coerce').fillna(0)
        df_test[target_col] = pd.to_numeric(df_test[target_col], errors='coerce').fillna(0)
    else:
        print(f"è­¦å‘Š: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•° '{target_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None, None, None, None

    # NaNã‚’å‰Šé™¤
    df_train_clean = df_train[feature_cols + [target_col]].dropna()
    df_test_clean = df_test[feature_cols + [target_col, 'material_key', 'file_date']].dropna()

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿç¸¾å€¤ã®åˆè¨ˆãŒã‚¼ãƒ­ã®material_keyã‚’é™¤å¤–
    print("\n=== ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ===")
    material_key_actual_sum = df_test_clean.groupby('material_key')[target_col].sum()
    material_keys_with_nonzero = material_key_actual_sum[material_key_actual_sum > 0].index

    original_test_size = len(df_test_clean)
    original_material_keys = df_test_clean['material_key'].nunique()

    df_test_clean = df_test_clean[df_test_clean['material_key'].isin(material_keys_with_nonzero)]

    filtered_test_size = len(df_test_clean)
    filtered_material_keys = df_test_clean['material_key'].nunique()

    print(f"å…ƒã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {original_test_size}è¡Œ, {original_material_keys} Material Keys")
    print(f"ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {filtered_test_size}è¡Œ, {filtered_material_keys} Material Keys")
    print(f"é™¤å¤–ã•ã‚ŒãŸMaterial Keys: {original_material_keys - filtered_material_keys}å€‹ï¼ˆå®Ÿç¸¾å€¤åˆè¨ˆãŒã‚¼ãƒ­ï¼‰")

    if len(df_train_clean) == 0 or len(df_test_clean) == 0:
        print("ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return None, None, None, None

    # ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    X_train = df_train_clean[feature_cols]
    y_train = df_train_clean[target_col]
    X_test = df_test_clean[feature_cols]
    y_test = df_test_clean[target_col]

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ã‚¿æƒ…å ±ã‚’ä¿æŒ
    test_meta = df_test_clean[['material_key', 'file_date']]

    # æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆ†å‰²
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42
    )

    # Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
    if is_optuna:
        print("\n=== Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ä¸­... ===")

        # æœ€é©åŒ–æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«
        cache_file = "optuna_best_params_cache.pkl"
        s3 = boto3.client('s3',
                          aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                          aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                          region_name=os.getenv('AWS_DEFAULT_REGION', 'ap-northeast-1'))
        bucket_name = "fiby-yamasa-prediction"

        # S3ã‹ã‚‰æœ€é©åŒ–æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        try:
            obj = s3.get_object(Bucket=bucket_name, Key="models/optuna_best_params.pkl")
            best_params = pickle.loads(obj['Body'].read())
            print("æœ€é©åŒ–æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’S3ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            print(f"ä½¿ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_params}")
        except:
            print("æ–°è¦ã«Optunaã§æœ€é©åŒ–ã‚’å®Ÿè¡Œã—ã¾ã™...")

            def objective(trial):
                params = {
                    'objective': 'regression',
                    'metric': 'rmse',
                    'num_leaves': trial.suggest_int('num_leaves', 20, 300),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
                    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
                    'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
                    'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                    'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
                    'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
                    'verbose': -1,
                    'n_estimators': 100,
                    'random_state': 42
                }

                model_trial = lgb.LGBMRegressor(**params)
                model_trial.fit(
                    X_tr, y_tr,
                    eval_set=[(X_val, y_val)],
                    callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
                )

                y_pred_val = model_trial.predict(X_val)
                rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))
                return rmse

            # Optunaã«ã‚ˆã‚‹æœ€é©åŒ–å®Ÿè¡Œ
            study = optuna.create_study(direction='minimize')
            study.optimize(objective, n_trials=20, show_progress_bar=True)  # è©¦è¡Œå›æ•°ã‚’20ã«å‰Šæ¸›

            best_params = study.best_params
            best_params['objective'] = 'regression'
            best_params['metric'] = 'rmse'
            best_params['verbose'] = -1
            best_params['n_estimators'] = 100
            best_params['random_state'] = 42

            print(f"\næœ€é©åŒ–å®Œäº†ï¼")
            print(f"æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_params}")
            print(f"æœ€è‰¯RMSE: {study.best_value:.4f}")

            # S3ã«æœ€é©åŒ–æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
            params_buffer = BytesIO()
            pickle.dump(best_params, params_buffer)
            params_buffer.seek(0)
            s3.put_object(
                Bucket=bucket_name,
                Key="models/optuna_best_params.pkl",
                Body=params_buffer.getvalue()
            )
            print("æœ€é©åŒ–æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’S3ã«ä¿å­˜ã—ã¾ã—ãŸ")

        params = best_params
    else:
        print("\nLightGBMãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­ï¼ˆOptunaãªã—ï¼‰...")
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'n_estimators': 100
        }

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    model = lgb.LGBMRegressor(**params)
    model.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],
        callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
    )

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
    y_pred = model.predict(X_test)

    # åŸºæœ¬çš„ãªè©•ä¾¡æŒ‡æ¨™
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)

    print(f"\n=== ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_start} ã€œ {test_end}ï¼‰===")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")

    # ç‰¹å¾´é‡é‡è¦åº¦
    importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    print(f"\n=== ç‰¹å¾´é‡é‡è¦åº¦ TOP10 ===")
    print(importance.head(10).to_string(index=False))

    # äºˆæ¸¬èª¤å·®åˆ†æ
    print(f"\n=== äºˆæ¸¬èª¤å·®åˆ†æ ===")
    df_results, df_key_date, df_key_total = calculate_prediction_errors(
        y_test, y_pred, test_meta
    )

    # èª¤å·®ç‡åˆ†å¸ƒã®åˆ†æ
    error_stats = analyze_error_distribution(df_key_total)

    # å…¨Material Keyæ•°ã‚’å–å¾—ï¼ˆå®Ÿç¸¾å€¤ãŒ0ã®ã‚‚ã®ã¯æ—¢ã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é™¤å¤–ã•ã‚Œã¦ã„ã‚‹ï¼‰
    total_all_materials = len(df_key_total)

    print(f"\n=== Material Keyæ¯ã®äºˆæ¸¬ç²¾åº¦åˆ†å¸ƒ ===")
    print(f"åˆ†æå¯¾è±¡Material Keyæ•°: {total_all_materials}å€‹")
    print(f"ï¼ˆæ³¨: å®Ÿç¸¾å€¤åˆè¨ˆãŒ0ã®Material Keyã¯äº‹å‰ã«é™¤å¤–æ¸ˆã¿ï¼‰")
    print(f"\näºˆæ¸¬ç²¾åº¦åˆ†å¸ƒ:")
    print(f"äºˆæ¸¬èª¤å·®20%ä»¥å†…: {error_stats['within_20_percent']}å€‹ ({error_stats['within_20_percent_ratio']:.1%})")
    print(f"äºˆæ¸¬èª¤å·®30%ä»¥å†…: {error_stats['within_30_percent']}å€‹ ({error_stats['within_30_percent_ratio']:.1%})")
    print(f"äºˆæ¸¬èª¤å·®50%ä»¥å†…: {error_stats['within_50_percent']}å€‹ ({error_stats['within_50_percent_ratio']:.1%})")
    print(f"\näºˆæ¸¬èª¤å·®ç‡ã®çµ±è¨ˆ:")
    print(f"å¹³å‡èª¤å·®ç‡: {error_stats['error_mean']*100:.2f}%")
    print(f"ä¸­å¤®èª¤å·®ç‡: {error_stats['error_median']*100:.2f}%")
    print(f"æ¨™æº–åå·®: {error_stats['error_std']*100:.2f}%")

    # æ”¹å–„çµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤ºï¼ˆå‰å›ã¨ã®æ¯”è¼ƒï¼‰
    print(f"\n=== ğŸ“Š äºˆæ¸¬ç²¾åº¦æ”¹å–„ã‚µãƒãƒªãƒ¼ï¼ˆæ–°ç‰¹å¾´é‡è¿½åŠ ã«ã‚ˆã‚‹åŠ¹æœï¼‰ ===")
    print(f"ã€æ”¹å–„å‰ â†’ æ”¹å–„å¾Œã€‘")
    print(f"äºˆæ¸¬èª¤å·®å¹³å‡: 965.96% â†’ {error_stats['error_mean']*100:.2f}%")
    print(f"äºˆæ¸¬èª¤å·®ä¸­å¤®å€¤: 224.47% â†’ {error_stats['error_median']*100:.2f}%")
    print(f"20%ä»¥å†…: 89å€‹ (17.8%) â†’ {error_stats['within_20_percent']}å€‹ ({error_stats['within_20_percent_ratio']:.1%})")
    print(f"30%ä»¥å†…: 115å€‹ (23.0%) â†’ {error_stats['within_30_percent']}å€‹ ({error_stats['within_30_percent_ratio']:.1%})")
    print(f"50%ä»¥å†…: 150å€‹ (30.0%) â†’ {error_stats['within_50_percent']}å€‹ ({error_stats['within_50_percent_ratio']:.1%})")

    # EVALå½¢å¼ã®æ–°ã—ã„è©•ä¾¡æŒ‡æ¨™ã‚’è¡¨ç¤º
    print(f"\n=== è©•ä¾¡æŒ‡æ¨™ï¼ˆEVALå½¢å¼ï¼‰ ===")
    print(f"[EVAL] mean(key_mean_abs_err_div_pred_overall) across material_key = {error_stats['eval_mean']:.6f} (n_keys={error_stats['eval_total_keys']})")
    print(f"[EVAL] median(key_mean_abs_err_div_pred_overall) across material_key = {error_stats['eval_median']:.6f} (n_keys={error_stats['eval_total_keys']})")
    print(f"[EVAL] #keys within 20%: {error_stats['eval_within_20']} / {error_stats['eval_total_keys']} ({error_stats['eval_within_20_ratio']*100:.2f}%)")
    print(f"[EVAL] #keys within 30%: {error_stats['eval_within_30']} / {error_stats['eval_total_keys']} ({error_stats['eval_within_30_ratio']*100:.2f}%)")
    print(f"[EVAL] #keys within 50%: {error_stats['eval_within_50']} / {error_stats['eval_total_keys']} ({error_stats['eval_within_50_ratio']*100:.2f}%)")

    # èª¤å·®ãŒå¤§ãã„Material Keyã®ä¾‹ã‚’è¡¨ç¤º
    df_key_total_sorted = df_key_total.copy()
    df_key_total_sorted['abs_error_rate_total'] = np.abs(df_key_total_sorted['error_rate_total'])
    df_key_total_sorted = df_key_total_sorted.sort_values('abs_error_rate_total', ascending=False)

    print(f"\n=== äºˆæ¸¬èª¤å·®ãŒå¤§ãã„Material Key TOP10 ===")
    print(df_key_total_sorted[['material_key', 'actual', 'predicted', 'error_rate_total', 'count_actual_positive_total']].head(10).to_string(index=False))

    print(f"\n=== äºˆæ¸¬ç²¾åº¦ãŒé«˜ã„Material Key TOP10 ===")
    df_key_total_sorted_asc = df_key_total_sorted[df_key_total_sorted['abs_error_rate_total'].notna()].sort_values('abs_error_rate_total')
    print(df_key_total_sorted_asc[['material_key', 'actual', 'predicted', 'error_rate_total', 'count_actual_positive_total']].head(10).to_string(index=False))

    return model, importance, {'rmse': rmse, 'mae': mae, 'error_stats': error_stats}, (df_results, df_key_date, df_key_total)

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""

    print("="*50)
    print("å­¦ç¿’ãƒ»äºˆæ¸¬å‡¦ç†é–‹å§‹ï¼ˆäºˆæ¸¬èª¤å·®åˆ†ææ©Ÿèƒ½ä»˜ãï¼‰")
    print("="*50)

    # S3ã‹ã‚‰ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    bucket_name = "fiby-yamasa-prediction"
    features_key = "features/df_features_yamasa_latest.parquet"

    print(f"\n1. ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿èª­è¾¼ä¸­...")
    print(f"ã‚½ãƒ¼ã‚¹: s3://{bucket_name}/{features_key}")

    # S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
    s3 = boto3.client('s3',
                      aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                      aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                      region_name=os.getenv('AWS_DEFAULT_REGION', 'ap-northeast-1'))

    try:
        # S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        obj = s3.get_object(Bucket=bucket_name, Key=features_key)
        df_features = pd.read_parquet(BytesIO(obj['Body'].read()))
        print(f"S3ã‹ã‚‰ã®èª­ã¿è¾¼ã¿æˆåŠŸ")
    except Exception as e:
        print(f"S3ã‹ã‚‰ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        local_file = "output_data/features/df_features_yamasa_latest.parquet"
        if os.path.exists(local_file):
            print(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿: {local_file}")
            df_features = pd.read_parquet(local_file)
        else:
            print("ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
    print(f"èª­è¾¼ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df_features.shape}")

    # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±ã‚’è¡¨ç¤º
    print(f"\n=== ãƒ‡ãƒ¼ã‚¿æƒ…å ± ===")
    print(f"è¡Œæ•°: {len(df_features):,}")
    print(f"ã‚«ãƒ©ãƒ æ•°: {len(df_features.columns)}")

    # Material Keyæ¯ã®ãƒ‡ãƒ¼ã‚¿æ•°
    if 'material_key' in df_features.columns:
        n_materials = df_features['material_key'].nunique()
        print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªMaterial Keyæ•°: {n_materials:,}")

    # usage_typeæ¯ã®material_keyæ•°ã‚’è¡¨ç¤º
    print("\n" + "="*50)
    print("usage_typeæ¯ã®material_keyæ•°:")
    print("="*50)
    if 'usage_type' in df_features.columns and 'material_key' in df_features.columns:
        usage_counts = df_features.groupby('usage_type')['material_key'].nunique()
        total_keys = df_features['material_key'].nunique()
        for usage, count in usage_counts.items():
            print(f"  {usage}: {count:,} material_keys ({count/total_keys*100:.1f}%)")
        print(f"  åˆè¨ˆ: {total_keys:,} material_keys")
    else:
        if 'usage_type' not in df_features.columns:
            print("  è­¦å‘Š: usage_typeã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        if 'material_key' not in df_features.columns:
            print("  è­¦å‘Š: material_keyã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

    # æ—¥ä»˜ç¯„å›²
    if 'file_date' in df_features.columns:
        df_features['file_date'] = pd.to_datetime(df_features['file_date'], errors='coerce')
        date_min = df_features['file_date'].min()
        date_max = df_features['file_date'].max()
        print(f"æ—¥ä»˜ç¯„å›²: {date_min} ã€œ {date_max}")

    # 2. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ãƒ»è©•ä¾¡
    print(f"\n2. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡é–‹å§‹...")

    model, importance, metrics, error_analysis = train_and_predict_with_test_period(
        df_features,
        test_start='2025-01-01',  # 6ãƒ¶æœˆï¼ˆ1æœˆã€œ6æœˆï¼‰
        test_end='2025-06-30',
        is_optuna=True  # Optunaã‚ã‚Šã§å®Ÿè¡Œ
    )

    if model is None:
        print("ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return None

    # 3. çµæœã‚’ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    date_dir = datetime.now().strftime('%Y%m%d')  # YYYYMMDDå½¢å¼ã®æ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    # S3ã«ä¿å­˜
    print("\nS3ã«çµæœã‚’ä¿å­˜ä¸­...")
    import joblib

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚¤ãƒˆåˆ—ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
    model_buffer = BytesIO()
    joblib.dump(model, model_buffer)
    model_buffer.seek(0)

    # S3ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆæ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä»˜ãï¼‰
    model_key = f"models/{date_dir}/model_with_error_{timestamp}.pkl"
    s3.put_object(
        Bucket=bucket_name,
        Key=model_key,
        Body=model_buffer.getvalue()
    )
    print(f"S3ãƒ¢ãƒ‡ãƒ«ä¿å­˜: s3://{bucket_name}/{model_key}")

    # æœ€æ–°ç‰ˆã¨ã—ã¦ã‚‚ä¿å­˜
    model_latest_key = "models/model_with_error_latest.pkl"
    s3.put_object(
        Bucket=bucket_name,
        Key=model_latest_key,
        Body=model_buffer.getvalue()
    )
    print(f"S3æœ€æ–°ãƒ¢ãƒ‡ãƒ«ä¿å­˜: s3://{bucket_name}/{model_latest_key}")

    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’S3ã«ä¿å­˜
    importance_buffer = BytesIO()
    importance.to_parquet(importance_buffer, index=False)
    importance_buffer.seek(0)
    importance_key = f"models/{date_dir}/importance_{timestamp}.parquet"
    s3.put_object(
        Bucket=bucket_name,
        Key=importance_key,
        Body=importance_buffer.getvalue()
    )
    print(f"S3ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜: s3://{bucket_name}/{importance_key}")

    # æœ€æ–°ç‰ˆã¨ã—ã¦ã‚‚ä¿å­˜
    importance_latest_key = "models/importance_with_error_latest.parquet"
    s3.put_object(
        Bucket=bucket_name,
        Key=importance_latest_key,
        Body=importance_buffer.getvalue()
    )
    print(f"S3æœ€æ–°ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜: s3://{bucket_name}/{importance_latest_key}")

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’S3ã«ä¿å­˜ï¼ˆerror_statsã‚’å«ã‚€å®Œå…¨ç‰ˆï¼‰
    metrics_full = metrics.copy()
    if 'error_stats' in metrics_full:
        # error_statsã‚’å±•é–‹ã—ã¦ãƒ•ãƒ©ãƒƒãƒˆãªæ§‹é€ ã«ã™ã‚‹
        error_stats = metrics_full.pop('error_stats')
        # int64ãªã©ã®NumPyå‹ã‚’Pythonæ¨™æº–å‹ã«å¤‰æ›
        for key, value in error_stats.items():
            if hasattr(value, 'item'):
                error_stats[key] = value.item()
        metrics_full.update(error_stats)

    # NumPyå‹ã‚’Pythonæ¨™æº–å‹ã«å¤‰æ›
    for key, value in metrics_full.items():
        if hasattr(value, 'item'):
            metrics_full[key] = value.item()

    metrics_buffer = BytesIO()
    import json
    metrics_json = json.dumps(metrics_full, indent=2)
    metrics_buffer.write(metrics_json.encode('utf-8'))
    metrics_buffer.seek(0)
    metrics_key = f"models/{date_dir}/metrics_with_error_{timestamp}.json"
    s3.put_object(
        Bucket=bucket_name,
        Key=metrics_key,
        Body=metrics_buffer.getvalue(),
        ContentType='application/json'
    )
    print(f"S3è©•ä¾¡æŒ‡æ¨™ä¿å­˜: s3://{bucket_name}/{metrics_key}")

    # æœ€æ–°ç‰ˆã¨ã—ã¦ã‚‚ä¿å­˜
    metrics_latest_key = "models/metrics_with_error_latest.json"
    s3.put_object(
        Bucket=bucket_name,
        Key=metrics_latest_key,
        Body=metrics_buffer.getvalue(),
        ContentType='application/json'
    )
    print(f"S3æœ€æ–°è©•ä¾¡æŒ‡æ¨™ä¿å­˜: s3://{bucket_name}/{metrics_latest_key}")

    # ã‚¨ãƒ©ãƒ¼çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚‚åˆ¥é€”ä¿å­˜
    if 'error_stats' in metrics:
        test_start = '2025-01-01'  # ãƒ†ã‚¹ãƒˆæœŸé–“ã®é–‹å§‹
        test_end = '2025-06-30'    # ãƒ†ã‚¹ãƒˆæœŸé–“ã®çµ‚äº†
        summary_stats = {
            'timestamp': timestamp,
            'test_period': f"{test_start} to {test_end}",
            'rmse': metrics['rmse'],
            'mae': metrics['mae'],
            'total_materials': metrics['error_stats']['total_materials'],
            'within_20_percent': metrics['error_stats']['within_20_percent'],
            'within_20_percent_ratio': metrics['error_stats']['within_20_percent_ratio'],
            'within_30_percent': metrics['error_stats']['within_30_percent'],
            'within_30_percent_ratio': metrics['error_stats']['within_30_percent_ratio'],
            'within_50_percent': metrics['error_stats']['within_50_percent'],
            'within_50_percent_ratio': metrics['error_stats']['within_50_percent_ratio'],
            'error_mean': metrics['error_stats']['error_mean'],
            'error_median': metrics['error_stats']['error_median'],
            'error_std': metrics['error_stats']['error_std']
        }

        summary_buffer = BytesIO()
        summary_json = json.dumps(summary_stats, indent=2)
        summary_buffer.write(summary_json.encode('utf-8'))
        summary_buffer.seek(0)
        summary_key = f"models/{date_dir}/error_summary_{timestamp}.json"
        s3.put_object(
            Bucket=bucket_name,
            Key=summary_key,
            Body=summary_buffer.getvalue(),
            ContentType='application/json'
        )
        print(f"S3ã‚¨ãƒ©ãƒ¼çµ±è¨ˆã‚µãƒãƒªãƒ¼ä¿å­˜: s3://{bucket_name}/{summary_key}")

        # æœ€æ–°ç‰ˆã¨ã—ã¦ã‚‚ä¿å­˜
        summary_latest_key = "models/error_summary_latest.json"
        s3.put_object(
            Bucket=bucket_name,
            Key=summary_latest_key,
            Body=summary_buffer.getvalue(),
            ContentType='application/json'
        )
        print(f"S3æœ€æ–°ã‚¨ãƒ©ãƒ¼çµ±è¨ˆã‚µãƒãƒªãƒ¼ä¿å­˜: s3://{bucket_name}/{summary_latest_key}")

    # èª¤å·®åˆ†æçµæœã‚’S3ã«ä¿å­˜
    if error_analysis:
        df_results, df_key_date, df_key_total = error_analysis

        # Material Key Ã— File Dateæ¯ã®çµæœ
        key_date_buffer = BytesIO()
        df_key_date.to_parquet(key_date_buffer, index=False)
        key_date_buffer.seek(0)  # ãƒãƒƒãƒ•ã‚¡ã®å…ˆé ­ã«æˆ»ã™
        key_date_key = f"models/{date_dir}/error_analysis_key_date_{timestamp}.parquet"
        s3.put_object(
            Bucket=bucket_name,
            Key=key_date_key,
            Body=key_date_buffer.getvalue(),
        )
        print(f"S3èª¤å·®åˆ†æ(KeyÃ—Date)ä¿å­˜: s3://{bucket_name}/{key_date_key}")

        # æœ€æ–°ç‰ˆã¨ã—ã¦ã‚‚ä¿å­˜
        key_date_latest_key = "models/error_analysis_key_date_latest.parquet"
        s3.put_object(
            Bucket=bucket_name,
            Key=key_date_latest_key,
            Body=key_date_buffer.getvalue(),
        )
        print(f"S3æœ€æ–°èª¤å·®åˆ†æ(KeyÃ—Date)ä¿å­˜: s3://{bucket_name}/{key_date_latest_key}")

        # Material Keyæ¯ã®çµæœï¼ˆ6ãƒ¶æœˆå…¨ä½“ï¼‰
        key_total_buffer = BytesIO()
        df_key_total.to_parquet(key_total_buffer, index=False)
        key_total_buffer.seek(0)  # ãƒãƒƒãƒ•ã‚¡ã®å…ˆé ­ã«æˆ»ã™
        key_total_key = f"models/{date_dir}/error_analysis_key_total_{timestamp}.parquet"
        s3.put_object(
            Bucket=bucket_name,
            Key=key_total_key,
            Body=key_total_buffer.getvalue(),
        )
        print(f"S3èª¤å·®åˆ†æ(Keyå…¨ä½“)ä¿å­˜: s3://{bucket_name}/{key_total_key}")

        # æœ€æ–°ç‰ˆã¨ã—ã¦ã‚‚ä¿å­˜
        key_total_latest_key = "models/error_analysis_key_total_latest.parquet"
        s3.put_object(
            Bucket=bucket_name,
            Key=key_total_latest_key,
            Body=key_total_buffer.getvalue(),
        )
        print(f"S3æœ€æ–°èª¤å·®åˆ†æ(Keyå…¨ä½“)ä¿å­˜: s3://{bucket_name}/{key_total_latest_key}")

        # è©³ç´°çµæœ
        results_buffer = BytesIO()
        df_results.to_parquet(results_buffer)
        results_key = f"models/{date_dir}/prediction_results_{timestamp}.parquet"
        s3.put_object(
            Bucket=bucket_name,
            Key=results_key,
            Body=results_buffer.getvalue()
        )
        print(f"S3äºˆæ¸¬çµæœè©³ç´°ä¿å­˜: s3://{bucket_name}/{results_key}")

        # æœ€æ–°ç‰ˆã¨ã—ã¦ã‚‚ä¿å­˜
        results_latest_key = "models/prediction_results_latest.parquet"
        s3.put_object(
            Bucket=bucket_name,
            Key=results_latest_key,
            Body=results_buffer.getvalue()
        )
        print(f"S3æœ€æ–°äºˆæ¸¬çµæœè©³ç´°ä¿å­˜: s3://{bucket_name}/{results_latest_key}")

    # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ã¯å‰Šé™¤ï¼ˆS3ã®ã¿ã«ä¿å­˜ï¼‰

    print("\n" + "="*50)
    print("å‡¦ç†å®Œäº†ï¼")
    print("="*50)

    return model, importance, metrics, error_analysis

if __name__ == "__main__":
    main()