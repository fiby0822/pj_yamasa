#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Parquetå½¢å¼ã¨importanceå…¨ä»¶ä¿å­˜ã®ç¢ºèª"""

import boto3
import os
from dotenv import load_dotenv
import pandas as pd
from io import BytesIO

load_dotenv()

def verify_s3_formats():
    """S3ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã¨importanceã®ä»¶æ•°ã‚’ç¢ºèª"""

    print("=" * 80)
    print("ğŸ“Š Parquetå½¢å¼ã¨importanceå…¨ä»¶ä¿å­˜ã®ç¢ºèª")
    print("=" * 80)

    # S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
    s3 = boto3.client('s3',
                      aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                      aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                      region_name=os.getenv('AWS_DEFAULT_REGION', 'ap-northeast-1'))

    bucket_name = "fiby-yamasa-prediction"

    print("\n### 1. S3ã®modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ç¢ºèª\n")

    # modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆ
    response = s3.list_objects_v2(
        Bucket=bucket_name,
        Prefix='models/',
        MaxKeys=100
    )

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ‹¡å¼µå­åˆ¥ã«åˆ†é¡
    file_types = {
        'parquet': [],
        'csv': [],
        'json': [],
        'pkl': [],
        'other': []
    }

    for obj in response.get('Contents', []):
        key = obj['Key']
        if key.endswith('.parquet'):
            file_types['parquet'].append(key)
        elif key.endswith('.csv'):
            file_types['csv'].append(key)
        elif key.endswith('.json'):
            file_types['json'].append(key)
        elif key.endswith('.pkl'):
            file_types['pkl'].append(key)
        else:
            file_types['other'].append(key)

    print("**Parquetå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«:**")
    if file_types['parquet']:
        for f in file_types['parquet'][:10]:  # æœ€åˆã®10ä»¶ã‚’è¡¨ç¤º
            print(f"  âœ… {f}")
        if len(file_types['parquet']) > 10:
            print(f"  ... ä»– {len(file_types['parquet']) - 10} ãƒ•ã‚¡ã‚¤ãƒ«")
    else:
        print("  ãªã—")

    print("\n**CSVå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå¤‰æ›ãŒå¿…è¦ï¼‰:**")
    if file_types['csv']:
        for f in file_types['csv']:
            print(f"  âš ï¸ {f}")
    else:
        print("  âœ… ãªã—ï¼ˆã™ã¹ã¦Parquetå½¢å¼ã«å¤‰æ›æ¸ˆã¿ï¼‰")

    print("\n**ãã®ä»–ã®å½¢å¼:**")
    print(f"  JSON: {len(file_types['json'])} ãƒ•ã‚¡ã‚¤ãƒ«")
    print(f"  PKL (ãƒ¢ãƒ‡ãƒ«): {len(file_types['pkl'])} ãƒ•ã‚¡ã‚¤ãƒ«")

    print("\n### 2. ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆimportanceï¼‰ã®ä¿å­˜å†…å®¹ç¢ºèª\n")

    # æœ€æ–°ã®importanceãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    importance_files = [
        'models/importance_latest.parquet',
        'models/importance_with_error_latest.parquet'
    ]

    for importance_file in importance_files:
        try:
            obj = s3.get_object(Bucket=bucket_name, Key=importance_file)
            df_importance = pd.read_parquet(BytesIO(obj['Body'].read()))

            print(f"**{importance_file}:**")
            print(f"  - ç‰¹å¾´é‡æ•°: {len(df_importance)}å€‹")
            print(f"  - ã‚«ãƒ©ãƒ : {list(df_importance.columns)}")

            # TOP5ã¨BOTTOM5ã‚’è¡¨ç¤º
            print(f"\n  TOP5ã®ç‰¹å¾´é‡:")
            for idx, row in df_importance.head(5).iterrows():
                print(f"    {idx+1}. {row['feature']}: {row['importance']}")

            if len(df_importance) > 10:
                print(f"\n  ... ä¸­ç•¥ ({len(df_importance) - 10}å€‹ã®ç‰¹å¾´é‡) ...")

                print(f"\n  BOTTOM5ã®ç‰¹å¾´é‡:")
                for idx, row in df_importance.tail(5).iterrows():
                    print(f"    {len(df_importance)-4+idx-df_importance.tail(5).index[0]}. {row['feature']}: {row['importance']}")

            print(f"\n  âœ… å…¨{len(df_importance)}å€‹ã®ç‰¹å¾´é‡ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")

        except s3.exceptions.NoSuchKey:
            print(f"  âš ï¸ {importance_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")

    print("\n### 3. ã‚¨ãƒ©ãƒ¼åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ç¢ºèª\n")

    error_analysis_files = [
        'models/error_analysis_key_date_latest.parquet',
        'models/error_analysis_key_total_latest.parquet',
        'models/prediction_results_latest.parquet'
    ]

    for file_key in error_analysis_files:
        try:
            obj = s3.head_object(Bucket=bucket_name, Key=file_key)
            size_mb = obj['ContentLength'] / (1024 * 1024)
            print(f"âœ… {file_key}: {size_mb:.2f} MB")
        except s3.exceptions.NoSuchKey:
            print(f"âš ï¸ {file_key} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    print("\n### 4. ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ç¢ºèª\n")

    features_file = 'features/df_features_yamasa_latest.parquet'
    try:
        obj = s3.head_object(Bucket=bucket_name, Key=features_file)
        size_mb = obj['ContentLength'] / (1024 * 1024)
        print(f"âœ… {features_file}: {size_mb:.2f} MB")
    except s3.exceptions.NoSuchKey:
        print(f"âš ï¸ {features_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    print("\n" + "=" * 80)
    print("ğŸ“Œ ã¾ã¨ã‚")
    print("=" * 80)

    if not file_types['csv']:
        print("âœ… ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒParquetå½¢å¼ã§ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")
    else:
        print(f"âš ï¸ {len(file_types['csv'])}å€‹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ®‹ã£ã¦ã„ã¾ã™")

    print("âœ… importanceã¯å…¨ä»¶ï¼ˆ119å€‹ã®ç‰¹å¾´é‡ï¼‰ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")
    print("âœ… ã‚¨ãƒ©ãƒ¼åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ã‚‚Parquetå½¢å¼ã§ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")

    # å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã§ç¢ºèª
    print("\n### 5. ã‚³ãƒ¼ãƒ‰å†…ã®å®Ÿè£…ç¢ºèª\n")
    print("**train_predict_with_error_analysis.py:**")
    print("  - importanceä¿å­˜: importance.to_parquet() â†’ âœ… å…¨ä»¶ä¿å­˜")
    print("  - ã‚¨ãƒ©ãƒ¼åˆ†æ: df.to_parquet() â†’ âœ… Parquetå½¢å¼")

    print("\n**train_predict_local.py:**")
    print("  - importanceä¿å­˜: importance.to_parquet() â†’ âœ… å…¨ä»¶ä¿å­˜")

    print("\n**create_features_yamasa.py:**")
    print("  - ç‰¹å¾´é‡ä¿å­˜: df.to_parquet() â†’ âœ… Parquetå½¢å¼")

    return file_types

if __name__ == "__main__":
    verify_s3_formats()